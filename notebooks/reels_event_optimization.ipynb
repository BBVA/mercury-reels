{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a24c8a-dede-4658-bccd-0db87cff718b",
   "metadata": {},
   "source": [
    "# REELS: Event optimizer\n",
    "\n",
    "<img style=\"float: right;\" src=\"images/piece-puzzle.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cd287-84f8-48d5-b47a-104fb0830de3",
   "metadata": {},
   "source": [
    "## Event Optimization Overview\n",
    "\n",
    "This notebook explores how to optimize event representations in `reels` to improve model performance and interpretability. You will learn how to:\n",
    "\n",
    "* Control event discovery using configuration parameters\n",
    "* Reduce high-cardinality event spaces\n",
    "* Evaluate the impact of different event settings on model performance\n",
    "* Compare optimized and non-optimized configurations\n",
    "\n",
    "The goal is to show how thoughtful event design can significantly influence both predictive quality and model structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778cd2b4-a293-450f-979a-43ffc3c7d780",
   "metadata": {},
   "source": [
    "## What is Event Optimization?\n",
    "\n",
    "In `reels`, event optimization refers to the process of refining how raw event codes are represented before model training. Real-world datasets often contain thousands of distinct event codes, many of which are sparse, redundant, or weakly informative.\n",
    "\n",
    "Event optimization reduces this high-cardinality space by:\n",
    "\n",
    "* Limiting the maximum number of discovered events\n",
    "* Selecting the most informative codes\n",
    "* Aggregating or pruning infrequent events\n",
    "\n",
    "The result is a more compact and balanced event representation, which can improve model stability, interpretability, and predictive performance while reducing noise introduced by rare or random codes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51496d5b-babc-45be-ad69-1e5ae66f619e",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "<img style=\"float: left;\" src=\"images/reels_small.png\"> You should complete the **reels_walkthrough** tutorial before starting with this one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb2c80-55e0-4273-86df-ba874e922a8c",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We will use some standard packages in this notebook that we include here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db30db-071c-450d-a5d9-c63a2bed201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, io\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0b29a-ed11-4ebe-b00d-53b8f25d6d8c",
   "metadata": {},
   "source": [
    "Now, we import reels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e7af0-9ac6-41b3-ad02-bc10fc2e4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea1aee-d4da-44e0-b812-c18e8147df17",
   "metadata": {},
   "source": [
    "Additionally, we can verify that we are using the right version. (This notebook requires at least version 1.3.1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac35a4-e588-409d-9ea9-fa9d188891be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reels.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab33474-96c4-4c32-9533-d28df463adfa",
   "metadata": {},
   "source": [
    "## The problem Event Optimization solves!\n",
    "\n",
    "Real datasets possibly have hundreds of millions of transactions, millions of clients and thousands of event codes. Reels usually works just fine in those conditions.\n",
    "\n",
    "On top of that, there may be additional complications:\n",
    "\n",
    "  - Sequences for each client are typically very long (over 100 events).\n",
    "  - We want to predict a target that is extremely unbalanced, like in fraud applications where only 1 in 10,000 clients are targets.\n",
    "\n",
    "Fitting the data will result in way too many sequences being seen just once in the dataset. That will result in extreme overfitting to the point that the model will be of very limited use when applied to unseen data. \n",
    "\n",
    "### What is the \"magical solution\" to that?\n",
    "\n",
    "In these conditions, we will typically fit the model with the `as_states` argument of `Targets.fit()` set to true. If, rather than having 30,000 event codes, we could \n",
    "\"magically\" select the 20 codes that are relevant to predicting the target and assign a dummy code to all the rest, we would benefit from both:\n",
    "\n",
    "  - The sequences becoming much shorter (as states)\n",
    "  - The sequences being much less unique\n",
    "  \n",
    "That produces a much smaller tree with many more visits per node that will generalize much better to unseen data.\n",
    "\n",
    "### What is the \"practical solution\" to that?\n",
    "\n",
    "Needless to say, it is hard to \"magically\" find **the 20** codes. When we limit the discovery of codes by setting the `max_num_events` argument when constructing `Events` objects, we are only limiting the event to the most **frequently seen** in the dataset, **not necessarily relevant** to any prediction.\n",
    "\n",
    "The good news is: we may find a set of 200 codes that includes 15 of them that works almost as well as the \"perfect\" fit.\n",
    "\n",
    "And we can do that, if we are lucky, by calling an automatic search with default parameters. More likely, it will happen using the tools the **event optimizer** provides to better understand the problem and manually including and excluding sets of codes from the model.\n",
    "\n",
    "### The quest for \"The Dictionary\"\n",
    "\n",
    "Summarizing, we want to find a dictionary that maps thousands of codes in an **Events** object into tens of relevant codes.\n",
    "\n",
    "This notebook provides examples to understand how to discover and apply that dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ad3c0-88a5-41c1-b15d-0f062f08d1c3",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "This synthetic dataset is \"loosely based\" on a real BBVA web navigation dataset. The original dataset cannot be used in the tutorial for compliance and practical considerations.\n",
    "\n",
    "This synthetic dataset is smaller, simpler and much less unbalanced than the original one. Therefore, it works much better \"out of the box\". Nonetheless, it does improve somewhat with event optimization and, because it is not so easy to auto detect the signal, provides a good basis to understand manual intervention on the optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb36a0c-c1c6-4c0d-aac9-f8a28b331ccd",
   "metadata": {},
   "source": [
    "The following code defines the function `create_datasets()` that uses a Clips object to read the test sequences from it and modifies them at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6dc82-39cc-4771-abb8-b5faa3efc26d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_chunk(seq, cli, time_ori, max_delay):\n",
    "\tN = len(seq)\n",
    "\n",
    "\temi    = ['click' for _ in range(N)]\n",
    "\tweight = [1 for _ in range(N)]\n",
    "\tclient = [cli for _ in range(N)]\n",
    "\ttimes  = []\n",
    "\tct = time_ori\n",
    "\n",
    "\tfor _ in range(N):\n",
    "\t\tct += random.random()*max_delay\n",
    "\t\ttimes.append(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ct)))\n",
    "\n",
    "\treturn pd.DataFrame(list(zip(emi, seq, weight, client, times)), columns = ['emitter', 'description', 'weight', 'client', 'time'])\n",
    "\n",
    "\n",
    "def modify_chunk(chunk, cli, reduce, mutate):\n",
    "\tN = chunk.shape[0]\n",
    "\n",
    "\tif N > 5:\n",
    "\t\tchunk = chunk.sample(frac = reduce, replace = False)\n",
    "\t\tN = chunk.shape[0]\n",
    "\n",
    "\tchunk.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\tfor _ in range(random.randrange(1, round(2 + mutate*N))):\n",
    "\t\ti = random.randrange(0, N)\n",
    "\t\tj = random.randrange(0, N)\n",
    "\t\tx = chunk.description.at[i] + 1\n",
    "\t\tchunk.description.at[i] = chunk.description.at[j]\n",
    "\t\tchunk.description.at[j] = x\n",
    "\n",
    "\tchunk.client = [cli for _ in range(N)]\n",
    "\n",
    "\treturn chunk\n",
    "\n",
    "\n",
    "def create_datasets(test = False, n_extra = 10, reduce = 0.95, mutate = 0.15):\n",
    "\n",
    "\tevents  = reels.Events()\n",
    "\tclients = reels.Clients()\n",
    "\tclips   = reels.Clips(clients, events)\n",
    "\n",
    "\tchunks = []\n",
    "\ttargs  = None\n",
    "\n",
    "\ttime_ori  = time.mktime(time.strptime('2022-06-01 00:00:01', '%Y-%m-%d %H:%M:%S'))\n",
    "\tmax_delay = 3600\n",
    "\n",
    "\tif test:\n",
    "\t\ti_base = 400\n",
    "\t\ti_top  = 500\n",
    "\telse:\n",
    "\t\ti_base = 0\n",
    "\t\ti_top  = 400\n",
    "\n",
    "\tfor i_cli in range(i_base, i_top):\n",
    "\t\tseq = clips.test_sequence(i_cli, False)\n",
    "\n",
    "\t\tcli = 'n%04i%03i' % (i_cli, 0)\n",
    "\n",
    "\t\tchunk = build_chunk(seq, cli, time_ori, max_delay)\n",
    "\t\tchunks.append(chunk)\n",
    "\n",
    "\t\tfor t in range(1, n_extra):\n",
    "\t\t\tcli\t  = 'n%04i%03i' % (i_cli, t)\n",
    "\t\t\tchunk = modify_chunk(chunk, cli, reduce, mutate)\n",
    "\t\t\tchunks.append(chunk)\n",
    "\n",
    "\t\tseq = clips.test_sequence(i_cli, True)\n",
    "\n",
    "\t\tcli = 't%04i%03i' % (i_cli, 0)\n",
    "\n",
    "\t\tchunk = build_chunk(seq, cli, time_ori, max_delay)\n",
    "\t\tchunks.append(chunk)\n",
    "\n",
    "\t\tct = time.mktime(time.strptime(chunk.time.iloc[-1], '%Y-%m-%d %H:%M:%S'))\n",
    "\t\tct += random.random()*max_delay\n",
    "\t\tct -= random.random()*max_delay\n",
    "\n",
    "\t\tchunk = pd.DataFrame([[cli, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ct))]], columns = ['client', 'time'])\n",
    "\t\ttargs = chunk if targs is None else pd.concat([targs, chunk])\n",
    "\n",
    "\treturn pd.concat(chunks), targs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5b907-b165-43f4-86b6-8058d03c559a",
   "metadata": {},
   "source": [
    "## A simple analysis\n",
    "\n",
    "We also copy from the **reels_walkthrough** tutorial the function `analyze()` that will serve to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e88e05-11e0-4b25-af7b-6b96079a9d43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze(targets, clips, targs):\n",
    "    target_hashes = set([clients.hash_client_id(str(id)) for id in targs.client])  # This is the set of all the clients who are targets\n",
    "    Y_obs = [int(hh in target_hashes) for hh in clips.clips_client_hashes()]       # This is the observed target/no_target for all the clients\n",
    "    \n",
    "    T = [t for t in targets.predict_clips(clips)]                                  # These are the predicted times\n",
    "    \n",
    "    t_copy = T.copy()\n",
    "    t_copy.sort()\n",
    "    t_cut = t_copy[sum(Y_obs)]                                                     # t_cut is a cutting time that generates the same number of targets.\n",
    "    \n",
    "    Y_pred = [int(t <= t_cut) for t in T]                                          # This is the predicted target/no_target for all the clients\n",
    "    \n",
    "    x_tab = pd.crosstab(pd.array(Y_obs), pd.array(Y_pred), rownames = ['Obs'], colnames = ['Pred'])\n",
    "    \n",
    "    acc    = metrics.accuracy_score(Y_obs, Y_pred)                                 # We compute basic metrics\n",
    "    prec   = metrics.precision_score(Y_obs, Y_pred)\n",
    "    f1     = metrics.f1_score(Y_obs, Y_pred)\n",
    "\n",
    "    print(x_tab)\n",
    "    print('Accuracy: %.3f, precision: %.3f, f1-score: %.3f' % (acc, prec, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1eb6ad-e805-41c6-bb41-34eb33ee3fff",
   "metadata": {},
   "source": [
    "We create the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b336cb1-da38-444e-ab1b-b0ce0f85b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_targs = create_datasets(False)\n",
    "test,  test_targs  = create_datasets(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681ccad-c997-4a30-9ba0-c56254f0b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a8a6a-8c88-4124-9a99-c44a7c306f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e5ccc-8953-45b3-b1c4-96eae68b44f7",
   "metadata": {},
   "source": [
    "And now, we perform a very simple cross-validated fit/predict as we learned in the **reels_walkthrough** tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502bc31-a710-4f25-971b-e0a2ac2c6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_train       = reels.Intake(train)\n",
    "intake_train_targs = reels.Intake(train_targs)\n",
    "intake_test        = reels.Intake(test)\n",
    "intake_test_targs  = reels.Intake(test_targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab82fd-9a8c-4f53-aa2b-5cd46bb3547e",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426209b3-659c-415c-9b80-90068911c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = reels.Events(max_num_events = 10000)\n",
    "\n",
    "intake_train.insert_rows(events)\n",
    "intake_test.insert_rows(events)\n",
    "\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d291e74-7ead-48b4-96d6-969023c29442",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9900535-1bbe-4edc-92ad-b0997ad23b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = reels.Clients()\n",
    "\n",
    "clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876308e-b6d5-43b0-891f-d9bf885ddb64",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82854c-3b9c-49e3-bf3a-4e7c1a3d8484",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clips = reels.Clips(clients, events)\n",
    "intake_train.scan_events(train_clips)\n",
    "\n",
    "train_clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6948f-f6d4-4195-8d20-10c14aac4d63",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2677cf-0498-4866-89a5-83673b0def26",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = reels.Targets(train_clips)\n",
    "intake_train_targs.insert_targets(targets)\n",
    "targets.fit(agg = 'longest', depth = 100, as_states = True)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d612c86-41f6-4e87-ab13-84f8afa9af50",
   "metadata": {},
   "source": [
    "We evaluate the prediction over the **training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81edaf5-3c8a-4fad-b74d-09a754f5f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(targets, train_clips, train_targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8101a54c-71dd-4486-a3ab-1e82a0dfae47",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9f68f-d5e6-4d4c-be3a-f6759286b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clips = reels.Clips(clients, events)\n",
    "intake_test.scan_events(test_clips)\n",
    "\n",
    "test_clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc6ef7-287f-482b-8ea5-4d88a7300d0c",
   "metadata": {},
   "source": [
    "And evaluate the prediction over the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c50e30-72dc-438a-ac26-bcdc833389bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(targets, test_clips, test_targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c685d-7875-41cb-94d3-2345fc10e58b",
   "metadata": {},
   "source": [
    "## Event Optimization\n",
    "\n",
    "So far, we have trained a rather good model that performs worse on test data, just as expected.\n",
    "\n",
    "We can also check some statistics about the nodes of the fitted tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd1fbba-ec83-45ab-aa5e-cea3f96c8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets.describe_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42fa5e-9d7b-4f37-8daa-e2830047d287",
   "metadata": {},
   "source": [
    "As we would expect, it has many nodes with only one visit and is somewhat unbalanced (about 8:1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf84e4c-45dd-4aa1-9f6f-4d2a5de82d56",
   "metadata": {},
   "source": [
    "In case the original data provides some business insight to us, since we assume the codes are just meaningless numbers,\n",
    "we are going to build a simple dictionary mapping the content to the code. In this case, we map the field `description` (that in our example contains just numbers, but could have been descriptive text.)\n",
    "\n",
    "Later we will add a column to our code exploration dataset using this.\n",
    "\n",
    "This can be done by just iterating through the object via `describe_events()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981ff36-41da-4869-9427-1d310b6ce173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "description = {}\n",
    "for emitter, descr, weight, code in events.describe_events():\n",
    "    description[code] = descr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687a09c-6b0d-4c8c-a7b7-7d7bf176ba46",
   "metadata": {},
   "source": [
    "Now, we make a copy of the events dataset and optimize the copy, since the optimization will alter the object and we are just running it to get some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe0299-fa0f-4c5d-9b94-4da56d16f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_copy = events.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fddf0-531a-49e1-bd8e-4c806a125e87",
   "metadata": {},
   "source": [
    "Now we run the `optimize_events()` method, just for one iteration and removing exponential decay and the confidence interval.\n",
    "\n",
    "The **exponential decay** is a parameter that allows weighting the score of a code less as it is found deeper in the tree.\n",
    "\n",
    "The **confidence interval** is computed exactly like in `Targets().fit()` and allows computing lift based on the lower bound of a binomial confidence interval for a proportion rather than the proportion itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6bd37-821a-48f9-b7b3-1a3732c2401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, dictionary, top_codes, log = events_copy.optimize_events(train_clips, targets, num_steps = 1, exp_decay = 0, lower_bound_p = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414fc1ec-3414-43df-b348-9968a1e92c10",
   "metadata": {},
   "source": [
    "It returns a tuple of 4 elements:\n",
    "\n",
    " 1. the dictionary that we are not going to use yet, \n",
    " 2. a boolean that is true on success,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21447c77-3a0d-4b41-a01b-f6dcb25b9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d89fd5-56e0-4527-85ba-fc6b2de6be30",
   "metadata": {},
   "source": [
    " 3. a log saved as a string giving us some idea about how the search went,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c41fb3-0c06-44e5-adad-2a84a0ccf8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562c530-e80e-49c5-a429-049d9819fdfd",
   "metadata": {},
   "source": [
    " 4. and `top_codes`, a dataframe (in csv format) with the performance of each code.\n",
    "    \n",
    "We convert it into a pandas dataframe by passing the string to `pd.read_csv()` using an `io.StringIO` and sort it by the column `n_incl_target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b0479-1131-4ba3-aafd-a7cb1b3f6a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top = pd.read_csv(io.StringIO(top_codes), sep = '\\t')\n",
    "top['description'] = [description[c] for c in top.code]\n",
    "top.sort_values(by = 'n_incl_target', ascending = False, inplace = True)\n",
    "top.reset_index(drop = True, inplace = True)\n",
    "\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb5674b-1cdd-47d1-9c14-2f5209e94e56",
   "metadata": {},
   "source": [
    "NOTE: We have added the field `description` from the original dataset in case it is more informative than the field code.\n",
    "\n",
    "The first four columns are the number of times seen vs target for both the node with the code (called \"incl\" for included) and for the parent node. The parent node is called \"succ\" (successor) because the tree is built in reverse time order. Therefore, the successor in the time sequence is the parent in the tree.\n",
    "\n",
    "The next two, `sum_dep` and `n_dep` provide an idea of how deep the code is found on average (== sum_dep/n_dep) in the sequence and can be used in the score when exponential decay is not zero.\n",
    "\n",
    "The next is `edf` (exponential decay factor) and multiplies the final score. E.g. If we set `exponential_decay` to 0.00693 it decays to approx 0.5 in 100 steps because (1- 0.00693)^100 is 0.4988.\n",
    "\n",
    "The next two are the proportion (or lower bound for the confidence interval) of target/seen for both \"incl\" and \"succ\" as explained above.\n",
    "\n",
    "The lift is the quotient of the two and can be log transformed depending on the argument `log_lift`.\n",
    "\n",
    "And the final score is the product `edf*prop_incl*lift`.\n",
    "\n",
    "This score is used by a greedy algorithm to select the codes that are considered for inclusion in decreasing score order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6942ed-9cc5-4b48-a66c-c2fc32f5428c",
   "metadata": {},
   "source": [
    "### Including/excluding codes\n",
    "\n",
    "We can use this dataset to manually include and exclude. Let's suppose we want all the codes with more than 100 targets included so that we don't risk leaving them out.\n",
    "We may also want to exclude every code that has never been part of a target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc497e6-1244-4b55-8aff-0d9e969ae636",
   "metadata": {},
   "outputs": [],
   "source": [
    "include = ','.join([str(x) for x in top.loc[top['n_incl_target'] > 100, 'code']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2442e90-12ce-407a-a07a-00d5adc36399",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ','.join([str(x) for x in top.loc[top['n_incl_target'] < 1, 'code']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb0cd4-d251-48d4-bc65-19159921c2dc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> Make sure you only pass comma separated strings of integers to the arguments <b>force_include</b> and <b>force_exclude</b>. A wrong type may crash your program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e73339-5a26-419a-9b80-ed04cb1b4559",
   "metadata": {},
   "source": [
    "Again, we make a copy and work with the copy since we don't want to modify `events` and the previous `events_copy` has been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd265ec-49a8-4fc9-9130-15c3fafde434",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_copy = events.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5168d949-3c62-4ab8-a215-edf742a3fefc",
   "metadata": {},
   "source": [
    "And we call `optimize_events()` again. This time with the included and excluded codes, 20 steps and trying to introduce 4 codes at each step. We leave the default arguments but set threshold to zero to accept any new code that results in improvement no matter how small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d4e0f-c59f-4110-8448-4eaaee46c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, dictionary, top_codes, log = events_copy.optimize_events(train_clips, targets, num_steps = 20, codes_per_step = 4, \n",
    "                                                                  force_include = include, force_exclude = exclude, threshold = 0, lower_bound_p = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252d68a-d0d3-4601-b9d9-a37fd4ee286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d5ad3-2b31-4dc5-b72e-4df98ce26991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4a01b-7d8a-46fb-b682-e286457d9f40",
   "metadata": {},
   "source": [
    "### How to apply the dictionary\n",
    "\n",
    "For the sake of this tutorial, we have obtained a dictionary that provides improvement. Of course, in a real case we can iterate and try different things combining the business insight provided by the description with the empirical data from the tree until we get our best results.\n",
    "\n",
    "At the moment, `events_copy` is already optimized and could be used. \n",
    "\n",
    "An alternative method is creating one by copying the original `events` and providing the dictionary to the `copy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1128ec-bfe6-483e-9f34-20a56688e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_events = events.copy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f0f0c-459e-4eca-b746-42e76cee38e2",
   "metadata": {},
   "source": [
    "And we use this `op_events` to create new clips and fit a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864fc4f-a7b8-461e-bbab-6d4d93ec3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clips = reels.Clips(clients, op_events)\n",
    "intake_train.scan_events(train_clips)\n",
    "\n",
    "train_clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68594a86-5999-49e5-9667-e0d3c47dc9b0",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476e713-f026-41fe-8941-9fb7b919bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = reels.Targets(train_clips)\n",
    "intake_train_targs.insert_targets(targets)\n",
    "targets.fit(agg = 'longest', depth = 100, as_states = True)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb47c88-2225-492a-8dc4-be8a373c7796",
   "metadata": {},
   "source": [
    "We can also check that the new tree has less nodes overall and many with fewer than one visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4299cb5-4fbe-4728-9a1b-c52628866419",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets.describe_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1266a4-61ef-441a-b521-b362992bab90",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12248d9-096e-4c84-9952-6592e48a8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(targets, train_clips, train_targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82be988-e305-4742-ab66-bc6381f51cb8",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb3852-c1ed-49f4-ab48-9f485e8ec5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clips = reels.Clips(clients, op_events)\n",
    "intake_test.scan_events(test_clips)\n",
    "test_clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042fd9a-8ea9-4a41-883f-8c3067af7235",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06842e-e68a-4d73-9464-501bcbc721be",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(targets, test_clips, test_targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081eed3-0b3f-41f7-a69f-bbca9c932e85",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note that:</b> The optimized model is smaller, builds a smaller tree and generalizes better on unseen data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f275e-9d5e-4d1b-bc7e-faf928ab2439",
   "metadata": {},
   "source": [
    "## Results Interpretation\n",
    "\n",
    "The comparison between the baseline configuration and the optimized event configuration shows clear structural and performance differences.\n",
    "\n",
    "When event discovery is unrestricted, the model builds a larger and more fragmented tree. Many nodes are supported by very few visits, which leads to sparsity and less reliable estimates. This is reflected in a more unbalanced structure and weaker generalization performance.\n",
    "\n",
    "After applying event optimization (e.g., limiting the number of discovered codes), the event space becomes more compact. The resulting tree is smaller, with more balanced node visits and stronger statistical support per split. Performance metrics improve or become more stable across cross-validation folds, indicating better generalization.\n",
    "\n",
    "In practical terms, these results show that reducing event cardinality does not simply shrink the model â€” it restructures it. The optimized configuration concentrates signal into fewer, more informative event codes, leading to a cleaner model structure and more reliable predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c82f8c-603e-4937-9a9f-6950969173ff",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "By the end of this notebook, you should understand that:\n",
    "\n",
    "* **Event representation strongly influences model behavior.** The way raw codes are defined and limited directly affects tree structure, sparsity, and predictive stability.\n",
    "* **High-cardinality event spaces can hurt generalization.** Too many rare or weakly supported codes lead to fragmented trees and unreliable estimates.\n",
    "* **Event optimization is a structural decision, not just a tuning trick.** Limiting or selecting events reshapes the model and concentrates signal into more informative splits.\n",
    "* **Simpler event spaces can improve both interpretability and performance.** A more compact representation often yields more balanced nodes and more stable cross-validated results.\n",
    "\n",
    "The main lesson is that thoughtful event design is a core part of modeling with `reels`, not a secondary refinement step.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
